# -*- coding: utf-8 -*-
"""Chatgpt Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vUwvmuBF0W7gRW1tl8rsfVPzMXVu5sjN
"""

import json
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from collections import Counter
from textblob import TextBlob
from wordcloud import WordCloud
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

STOPWORDS = {
    'with', 'this', 'from', 'that', 'your', 'self', 'the', 'for', 'are', 'you', 'can', 'how',
    'what', 'when', 'where', 'who', 'which', 'and', 'or', 'but', 'not', 'all', 'any', 'time',
    'https', 'http', 'www', 'com', 'org', 'net', 'like', 'have', 'has', 'had', 'get', 'use',
    'also', 'will', 'just', 'more', 'only', 'very', 'should', 'could', 'would', 'need'
}

with open("/content/conversations.json", "r", encoding="utf-8") as f:
    data = json.load(f)
messages = []
for chat in data:
    title = chat.get("title", "Untitled")
    create_time = chat.get("create_time", 0)
    for item in chat["mapping"].values():
        msg = item.get("message")
        if msg and msg.get("author", {}).get("role") == "user":
            parts = msg.get("content", {}).get("parts", [])
            if parts and isinstance(parts[0], str):
                messages.append({
                    'text': parts[0],
                    'timestamp': create_time,
                    'title': title
                })

df = pd.DataFrame(messages)
df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
df['date'] = df['timestamp'].dt.date
df['hour'] = df['timestamp'].dt.hour
df['day_name'] = df['timestamp'].dt.day_name()
df['weekday'] = df['timestamp'].dt.weekday
df['week'] = df['timestamp'].dt.isocalendar().week
df['month'] = df['timestamp'].dt.month

print(f"{len(df)} messages loaded - {len(df['date'].unique())} days")

def extract_meaningful_words(text):
    if not isinstance(text, str): return []
    words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
    return [w for w in words if w not in STOPWORDS]

df['word_count'] = df['text'].str.split().str.len()
df['char_count'] = df['text'].str.len()
df['words'] = df['text'].apply(extract_meaningful_words)
df['meaningful_word_count'] = df['words'].str.len()

all_meaningful_words = [w for sublist in df['words'] for w in sublist]
word_freq = Counter(all_meaningful_words)

df['sentiment'] = df['text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity if isinstance(x, str) else 0)
df['sentiment_label'] = df['sentiment'].apply(lambda x: 'POSITIVE' if x > 0.1 else ('NEGATIVE' if x < -0.1 else 'NEUTRAL'))

topics = {
    'DATA_SCIENCE': ['data', 'project', 'code', 'python', 'pandas', 'json', 'script', 'pycharm', 'analysis', 'eda'],
    'FINANCE': ['salary', 'money', 'emi', 'cost', 'price', 'worth', 'buy', 'iphone', 'investment'],
    'LEARNING': ['learn', 'tutorial', 'explain', 'teach', 'understand'],
    'PERSONAL': ['mind', 'focus', 'brain', 'life', 'feel', 'think'],
    'TECH': ['react', 'node', 'mysql', 'docker', 'github', 'colab', 'api']
}

def get_topic(text):
    if not isinstance(text, str): return 'OTHER'
    text_lower = text.lower()
    for topic, keywords in topics.items():
        if any(kw in text_lower for kw in keywords):
            return topic
    return 'OTHER'

df['topic'] = df['text'].apply(get_topic)

focus_weights = {'DATA_SCIENCE': 3, 'LEARNING': 2.5, 'TECH': 2, 'FINANCE': 1.5, 'PERSONAL': 1, 'OTHER': 0}
df['focus_score'] = df['topic'].map(focus_weights).fillna(0)
df['productivity_score'] = df['focus_score'] * (1 + df['sentiment']/2)  # Sentiment boost

keyword_obsessions = {}
for word, count in word_freq.most_common(100):
    if count >= 5:  # Only words repeated 5+ times
        keyword_obsessions[word] = count

hourly_stats = df.groupby('hour').agg({
    'text': 'count',
    'focus_score': 'mean',
    'sentiment': 'mean',
    'word_count': 'mean'
}).rename(columns={'text': 'msg_count'})

daily_stats = df.groupby('date').agg({
    'text': 'count',
    'focus_score': 'mean',
    'sentiment': 'mean'
}).rename(columns={'text': 'msg_count'})

topic_sentiment = df.groupby('topic')['sentiment'].agg(['mean', 'std', 'count'])
topic_focus = df.groupby('topic')['focus_score'].mean()

stress_keywords = ['error', 'problem', 'stuck', 'confused', 'difficult', 'issue', 'bug']
df['stress_level'] = df['text'].apply(
    lambda x: sum(1 for kw in stress_keywords if kw in str(x).lower())
    + max(0, -df.loc[df['text']==x, 'sentiment'].values[0] if len(df.loc[df['text']==x, 'sentiment'].values) > 0 else 0)
)

df['is_deep'] = df['word_count'] > df['word_count'].quantile(0.75)
deep_vs_shallow = {
    'Deep (>75th percentile)': len(df[df['is_deep']]),
    'Shallow (<75th percentile)': len(df[~df['is_deep']])
}

print(f"\n1ï¸ OVERALL STATS:")
print(f"  Total messages: {len(df):,}")
print(f"  Date range: {df['timestamp'].min().date()} to {df['timestamp'].max().date()}")
print(f"  Avg words/message: {df['word_count'].mean():.1f}")
print(f"  Avg meaningful words: {df['meaningful_word_count'].mean():.1f}")

print(f"\n2ï¸ TOP 25 MEANINGFUL KEYWORDS:")
print("="*50)
for i, (word, count) in enumerate(word_freq.most_common(25), 1):
    pct = (count / len(all_meaningful_words)) * 100
    print(f"  {i:2d}. {word.upper():15s} {count:4d} ({pct:4.1f}%)")

print(f"\n3ï¸ KEYWORD OBSESSIONS (Repeated 5+ times):")

sorted_obsessions = sorted(keyword_obsessions.items(), key=lambda x: x[1], reverse=True)
for word, count in sorted_obsessions[:15]:
    print(f"  {word.upper():15s}: {count:3d} times")

print(f"\n4ï¸ SENTIMENT ANALYSIS:")
sentiment_pct = df['sentiment_label'].value_counts(normalize=True) * 100
for sent, pct in sentiment_pct.items():
    count = len(df[df['sentiment_label'] == sent])
    print(f"  {sent:10s}: {pct:5.1f}% ({count:,} messages)")
print(f"  Avg sentiment: {df['sentiment'].mean():.2f} (range: -1 to 1)")

print(f"\n5ï¸ TOPIC BREAKDOWN:")

for topic in df['topic'].value_counts().index:
    count = len(df[df['topic'] == topic])
    pct = (count / len(df)) * 100
    avg_sentiment = df[df['topic'] == topic]['sentiment'].mean()
    print(f"  {topic:12s}: {count:4d} ({pct:5.1f}%) | Avg sentiment: {avg_sentiment:+.2f}")

print(f"\n6ï¸ PATTERNS:")
peak_hour = hourly_stats['msg_count'].idxmax()
peak_focus_hour = hourly_stats['focus_score'].idxmax()
print(f"  Peak thinking hour: {peak_hour}:00 ({int(hourly_stats.loc[peak_hour, 'msg_count'])} messages)")
print(f"  Most productive hour: {peak_focus_hour}:00 (focus: {hourly_stats.loc[peak_focus_hour, 'focus_score']:.2f})")
print(f"  Best focus day: {df.groupby('day_name')['focus_score'].mean().idxmax()}")

print(f"\n7ï¸ MESSAGE DEPTH:")
print(f"  Deep messages: {deep_vs_shallow['Deep (>75th percentile)']:,} ({deep_vs_shallow['Deep (>75th percentile)']/len(df)*100:.1f}%)")
print(f"  Shallow messages: {deep_vs_shallow['Shallow (<75th percentile)']:,} ({deep_vs_shallow['Shallow (<75th percentile)']/len(df)*100:.1f}%)")
print(f"  Avg focus score: {df['focus_score'].mean():.2f}/3.0")

print(f"\n8ï¸ STRESS INDICATORS:")
high_stress = len(df[df['stress_level'] > df['stress_level'].quantile(0.75)])
print(f"  High-stress messages: {high_stress} ({high_stress/len(df)*100:.1f}%)")
print(f"  Most stressful topic: {df.groupby('topic')['sentiment'].mean().idxmin()} (lowest sentiment)")

print(f"\n9ï¸ LEARNING PROGRESSION:")
msgs_per_week = df.groupby('week').size()
if len(msgs_per_week) > 1:
    trend = stats.linregress(range(len(msgs_per_week)), msgs_per_week.values)
    trend_direction = "INCREASING" if trend.slope > 0 else " DECREASING"
    print(f"  Weekly activity trend: {trend_direction}")
    print(f"  Correlation: {trend.rvalue:.2f}")

print(f"\n10 TOP TOPIC-SENTIMENT CORRELATIONS:")
for topic in df['topic'].unique():
    topic_data = df[df['topic'] == topic]
    avg_sent = topic_data['sentiment'].mean()
    emoji = "ðŸ˜Š" if avg_sent > 0.1 else ("ðŸ˜¢" if avg_sent < -0.1 else "ðŸ˜")
    print(f"  {emoji} {topic:15s}: {avg_sent:+.3f}")

# VISUALIZATIONS (6 Advanced Charts)
fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)
# 1. Hourly Focus Trend
ax1 = fig.add_subplot(gs[0, 0])
hourly_stats['focus_score'].plot(ax=ax1, marker='o', linewidth=2, color='gold')
ax1.fill_between(hourly_stats.index, hourly_stats['focus_score'], alpha=0.3)
ax1.set_title('â­ Hourly Focus Trend', fontsize=12, color='white')
ax1.set_xlabel('Hour')

fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)
# 2. Daily Activity Heatmap (Last 60 days)
ax2 = fig.add_subplot(gs[0, 1])
recent_daily = daily_stats.tail(60)['msg_count']
ax2.bar(range(len(recent_daily)), recent_daily.values, color='cyan', alpha=0.8)
ax2.set_title('Daily Activity (Last 60 days)', fontsize=12, color='white')

fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)
# 3. Topic Distribution Pie
ax3 = fig.add_subplot(gs[0, 2])
topic_counts = df['topic'].value_counts()
ax3.pie(topic_counts.values, labels=topic_counts.index, autopct='%1.1f%%')
ax3.set_title('Mind Topics', fontsize=12, color='white')

fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)
# 4. Sentiment vs Focus Scatter
ax4 = fig.add_subplot(gs[1, 0])
ax4.scatter(df['sentiment'], df['focus_score'], alpha=0.5, s=30, c=df['word_count'], cmap='viridis')
ax4.set_xlabel('Sentiment')
ax4.set_ylabel('Focus Score')
ax4.set_title('ðŸ“Š Sentiment vs Focus', fontsize=12, color='white')
plt.colorbar(ax4.collections[0], ax=ax4, label='Word Count')

fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)
# 5. Deep vs Shallow Messages
ax5 = fig.add_subplot(gs[1, 1])
bars = ax5.bar(deep_vs_shallow.keys(), deep_vs_shallow.values(), color=['gold', 'cyan'], alpha=0.8)
ax5.set_title('ðŸ“ Deep vs Shallow Thinking', fontsize=12, color='white')
for bar in bars:
    height = bar.get_height()
    ax5.text(bar.get_x() + bar.get_width()/2., height + 50,
             f'{int(height):,}', ha='center', va='bottom')

fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)
# 6. Weekly Activity Pattern
ax6 = fig.add_subplot(gs[1, 2])
weekly = df.groupby('day_name')['text'].count()
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
weekly = weekly.reindex([d for d in day_order if d in weekly.index])
ax6.bar(range(len(weekly)), weekly.values, color='purple', alpha=0.8)
ax6.set_xticks(range(len(weekly)))
ax6.set_xticklabels([d[:3] for d in weekly.index], rotation=45)
ax6.set_title('ðŸ“† Weekly Pattern', fontsize=12, color='white')

fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)
# 7. Top Keywords Bar
ax7 = fig.add_subplot(gs[2, 0])
top_15 = word_freq.most_common(15)
words, counts = zip(*top_15)
ax7.barh(range(len(words)), counts, color='lime')
ax7.set_yticks(range(len(words)))
ax7.set_yticklabels([w.upper() for w in words])
ax7.set_title('Top 15 Keywords', fontsize=12, color='white')

fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)
# 8. Obsessions (5+ repeats)
ax8 = fig.add_subplot(gs[2, 1])
obsession_words = list(keyword_obsessions.keys())[:10]
obsession_counts = [keyword_obsessions[w] for w in obsession_words]
ax8.barh(range(len(obsession_words)), obsession_counts, color='orange')
ax8.set_yticks(range(len(obsession_words)))
ax8.set_yticklabels([w.upper() for w in obsession_words])
ax8.set_title('Brain Obsessions', fontsize=12, color='white')

fig = plt.figure(figsize=(20, 16))
gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)
# 9. WordCloud
ax9 = fig.add_subplot(gs[2, 2])
ax9.set_facecolor('black')
if len(all_meaningful_words) > 20:
    wc = WordCloud(width=400, height=400, background_color='black',
                   max_words=60, colormap='plasma').generate_from_frequencies(dict(word_freq.most_common(60)))
    ax9.imshow(wc, interpolation='bilinear')
ax9.axis('off')
ax9.set_title(' Brain Cloud', fontsize=12, color='white')





